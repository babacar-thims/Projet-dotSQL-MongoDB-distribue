2023-03-13T13:57:10.372+0000 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
2023-03-13T13:57:10.863+0000 W  ASIO     [main] No TransportLayer configured during NetworkInterface startup
2023-03-13T13:57:10.864+0000 I  CONTROL  [initandlisten] MongoDB starting : pid=6268 port=27023 dbpath=\data\rs3 64-bit host=DESKTOP-9V258QU
2023-03-13T13:57:10.864+0000 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
2023-03-13T13:57:10.864+0000 I  CONTROL  [initandlisten] db version v4.2.24
2023-03-13T13:57:10.865+0000 I  CONTROL  [initandlisten] git version: 5e4ec1d24431fcdd28b579a024c5c801b8cde4e2
2023-03-13T13:57:10.865+0000 I  CONTROL  [initandlisten] allocator: tcmalloc
2023-03-13T13:57:10.873+0000 I  CONTROL  [initandlisten] modules: none
2023-03-13T13:57:10.873+0000 I  CONTROL  [initandlisten] build environment:
2023-03-13T13:57:10.873+0000 I  CONTROL  [initandlisten]     distmod: 2012plus
2023-03-13T13:57:10.873+0000 I  CONTROL  [initandlisten]     distarch: x86_64
2023-03-13T13:57:10.873+0000 I  CONTROL  [initandlisten]     target_arch: x86_64
2023-03-13T13:57:10.873+0000 I  CONTROL  [initandlisten] options: { net: { port: 27023 }, replication: { replSet: "myRepl" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "\data\rs3" }, systemLog: { destination: "file", path: "\data\rs3\3.log" } }
2023-03-13T13:57:10.875+0000 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3528M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000,close_scan_interval=10,close_handle_minimum=250),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
2023-03-13T13:57:10.929+0000 I  STORAGE  [initandlisten] WiredTiger message [1678715830:929578][6268:140725933595440], txn-recover: Set global recovery timestamp: (0, 0)
2023-03-13T13:57:10.941+0000 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
2023-03-13T13:57:10.953+0000 I  STORAGE  [initandlisten] Timestamp monitor starting
2023-03-13T13:57:10.961+0000 I  CONTROL  [initandlisten] 
2023-03-13T13:57:10.961+0000 I  CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2023-03-13T13:57:10.961+0000 I  CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2023-03-13T13:57:10.962+0000 I  CONTROL  [initandlisten] 
2023-03-13T13:57:10.962+0000 I  CONTROL  [initandlisten] ** WARNING: This server is bound to localhost.
2023-03-13T13:57:10.962+0000 I  CONTROL  [initandlisten] **          Remote systems will be unable to connect to this server. 
2023-03-13T13:57:10.962+0000 I  CONTROL  [initandlisten] **          Start the server with --bind_ip <address> to specify which IP 
2023-03-13T13:57:10.962+0000 I  CONTROL  [initandlisten] **          addresses it should serve responses from, or with --bind_ip_all to
2023-03-13T13:57:10.962+0000 I  CONTROL  [initandlisten] **          bind to all interfaces. If this behavior is desired, start the
2023-03-13T13:57:10.962+0000 I  CONTROL  [initandlisten] **          server with --bind_ip 127.0.0.1 to disable this warning.
2023-03-13T13:57:10.962+0000 I  CONTROL  [initandlisten] 
2023-03-13T13:57:10.964+0000 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
2023-03-13T13:57:10.964+0000 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: d804abef-f504-4ec2-8161-8b809f3c54c4 and options: { capped: true, size: 10485760 }
2023-03-13T13:57:10.976+0000 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
2023-03-13T13:57:11.578+0000 W  FTDC     [initandlisten] Failed to initialize Performance Counters for FTDC: WindowsPdhError: PdhExpandCounterPathW failed with 'L’objet spécifié n’a pas été trouvé sur l’ordinateur.' for counter '\Memory\Available Bytes'
2023-03-13T13:57:11.578+0000 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/data/rs3/diagnostic.data'
2023-03-13T13:57:11.581+0000 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: ReadConcernMajorityNotAvailableYet: could not get updated shard list from config server :: caused by :: Read concern majority reads are currently not possible.; will retry after 30s
2023-03-13T13:57:11.581+0000 I  SHARDING [thread1] creating distributed lock ping thread for process ConfigServer (sleeping for 30000ms)
2023-03-13T13:57:11.583+0000 I  STORAGE  [initandlisten] createCollection: local.replset.oplogTruncateAfterPoint with generated UUID: 170cc23a-f375-41cf-a589-fa555e361a37 and options: {}
2023-03-13T13:57:11.604+0000 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.oplogTruncateAfterPoint
2023-03-13T13:57:11.604+0000 I  STORAGE  [initandlisten] createCollection: local.replset.minvalid with generated UUID: 287ac6b7-94ee-4b3e-b2d3-ec16c705f191 and options: {}
2023-03-13T13:57:11.621+0000 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.minvalid
2023-03-13T13:57:11.621+0000 I  STORAGE  [initandlisten] createCollection: local.replset.election with generated UUID: f9d3261f-bf1b-4bb2-8200-e202c334c738 and options: {}
2023-03-13T13:57:11.637+0000 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.election
2023-03-13T13:57:11.637+0000 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
2023-03-13T13:57:11.637+0000 I  REPL     [initandlisten] Did not find local Rollback ID document at startup. Creating one.
2023-03-13T13:57:11.638+0000 I  STORAGE  [initandlisten] createCollection: local.system.rollback.id with generated UUID: 7577c2cf-1e01-41be-8f06-46184aeb6e22 and options: {}
2023-03-13T13:57:11.653+0000 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.system.rollback.id
2023-03-13T13:57:11.653+0000 I  REPL     [initandlisten] Initialized the rollback ID to 1
2023-03-13T13:57:11.653+0000 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
2023-03-13T13:57:11.657+0000 I  SH_REFR  [ConfigServerCatalogCacheLoader-0] Refresh for database config from version {} to version { uuid: UUID("1f6967bb-8b10-4714-8a17-c9d1123eaf51"), lastMod: 0 } took 0 ms
2023-03-13T13:57:11.657+0000 I  CONTROL  [LogicalSessionCacheRefresh] Failed to create config.system.sessions: Cannot create config.system.sessions until there are shards, will try again at the next refresh interval
2023-03-13T13:57:11.657+0000 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: Cannot use non-local read concern until replica set is finished initializing.
2023-03-13T13:57:11.657+0000 I  NETWORK  [listener] Listening on 127.0.0.1
2023-03-13T13:57:11.658+0000 I  NETWORK  [listener] waiting for connections on port 27023
2023-03-13T13:57:11.657+0000 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Cannot create config.system.sessions until there are shards
2023-03-13T13:57:41.582+0000 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: NotYetInitialized: could not get updated shard list from config server :: caused by :: Cannot use non-local read concern until replica set is finished initializing.; will retry after 30s
2023-03-13T13:58:11.582+0000 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: NotYetInitialized: could not get updated shard list from config server :: caused by :: Cannot use non-local read concern until replica set is finished initializing.; will retry after 30s
2023-03-13T13:58:41.583+0000 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: NotYetInitialized: could not get updated shard list from config server :: caused by :: Cannot use non-local read concern until replica set is finished initializing.; will retry after 30s
2023-03-13T13:59:11.583+0000 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: NotYetInitialized: could not get updated shard list from config server :: caused by :: Cannot use non-local read concern until replica set is finished initializing.; will retry after 30s
2023-03-13T13:59:41.584+0000 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: NotYetInitialized: could not get updated shard list from config server :: caused by :: Cannot use non-local read concern until replica set is finished initializing.; will retry after 30s
2023-03-13T14:00:11.585+0000 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: NotYetInitialized: could not get updated shard list from config server :: caused by :: Cannot use non-local read concern until replica set is finished initializing.; will retry after 30s
2023-03-13T14:00:41.585+0000 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: NotYetInitialized: could not get updated shard list from config server :: caused by :: Cannot use non-local read concern until replica set is finished initializing.; will retry after 30s
2023-03-13T14:01:11.586+0000 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: NotYetInitialized: could not get updated shard list from config server :: caused by :: Cannot use non-local read concern until replica set is finished initializing.; will retry after 30s
2023-03-13T14:01:41.587+0000 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: NotYetInitialized: could not get updated shard list from config server :: caused by :: Cannot use non-local read concern until replica set is finished initializing.; will retry after 30s
2023-03-13T14:02:11.588+0000 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: NotYetInitialized: could not get updated shard list from config server :: caused by :: Cannot use non-local read concern until replica set is finished initializing.; will retry after 30s
2023-03-13T14:02:11.657+0000 I  CONTROL  [LogicalSessionCacheRefresh] Failed to create config.system.sessions: Cannot create config.system.sessions until there are shards, will try again at the next refresh interval
2023-03-13T14:02:11.658+0000 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: Cannot use non-local read concern until replica set is finished initializing.
2023-03-13T14:02:11.658+0000 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Cannot create config.system.sessions until there are shards
2023-03-13T14:02:41.590+0000 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: NotYetInitialized: could not get updated shard list from config server :: caused by :: Cannot use non-local read concern until replica set is finished initializing.; will retry after 30s
2023-03-13T14:03:11.591+0000 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: NotYetInitialized: could not get updated shard list from config server :: caused by :: Cannot use non-local read concern until replica set is finished initializing.; will retry after 30s
2023-03-13T14:03:41.592+0000 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: NotYetInitialized: could not get updated shard list from config server :: caused by :: Cannot use non-local read concern until replica set is finished initializing.; will retry after 30s
2023-03-13T14:04:11.592+0000 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: NotYetInitialized: could not get updated shard list from config server :: caused by :: Cannot use non-local read concern until replica set is finished initializing.; will retry after 30s
2023-03-13T14:04:41.593+0000 I  SHARDING [shard-registry-reload] Periodic reload of shard registry failed  :: caused by :: NotYetInitialized: could not get updated shard list from config server :: caused by :: Cannot use non-local read concern until replica set is finished initializing.; will retry after 30s
2023-03-13T14:04:45.208+0000 I  NETWORK  [listener] connection accepted from 127.0.0.1:51913 #1 (1 connection now open)
2023-03-13T14:04:45.209+0000 I  NETWORK  [conn1] end connection 127.0.0.1:51913 (0 connections now open)
2023-03-13T14:04:45.220+0000 I  NETWORK  [listener] connection accepted from 127.0.0.1:51915 #2 (1 connection now open)
2023-03-13T14:04:45.221+0000 I  NETWORK  [conn2] received client metadata from 127.0.0.1:51915 conn2: { driver: { name: "NetworkInterfaceTL", version: "4.2.24" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
2023-03-13T14:04:45.223+0000 I  CONNPOOL [Replication] Connecting to localhost:27021
2023-03-13T14:04:45.749+0000 I  STORAGE  [replexec-1] createCollection: local.system.replset with generated UUID: 31065a5a-c5ee-46f1-9c71-306d19568f42 and options: {}
2023-03-13T14:04:45.758+0000 I  NETWORK  [listener] connection accepted from 127.0.0.1:51921 #6 (2 connections now open)
2023-03-13T14:04:45.760+0000 I  NETWORK  [conn6] end connection 127.0.0.1:51921 (1 connection now open)
2023-03-13T14:04:45.777+0000 I  INDEX    [replexec-1] index build: done building index _id_ on ns local.system.replset
2023-03-13T14:04:45.778+0000 I  REPL     [replexec-1] New replica set config in use: { _id: "myRepl", version: 1, configsvr: true, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:27021", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "localhost:27022", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: "localhost:27023", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('640f2d7dd54f47b778949376') } }
2023-03-13T14:04:45.778+0000 I  REPL     [replexec-1] This node is localhost:27023 in the config
2023-03-13T14:04:45.778+0000 I  REPL     [replexec-1] transition to STARTUP2 from STARTUP
2023-03-13T14:04:45.779+0000 I  REPL     [replexec-1] Starting replication storage threads
2023-03-13T14:04:45.779+0000 I  CONNPOOL [Replication] Connecting to localhost:27022
2023-03-13T14:04:45.780+0000 I  REPL     [replexec-3] Member localhost:27021 is now in state SECONDARY
2023-03-13T14:04:45.783+0000 I  NETWORK  [listener] connection accepted from 127.0.0.1:51923 #8 (2 connections now open)
2023-03-13T14:04:45.783+0000 I  NETWORK  [conn8] received client metadata from 127.0.0.1:51923 conn8: { driver: { name: "NetworkInterfaceTL", version: "4.2.24" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
2023-03-13T14:04:45.784+0000 I  REPL     [replexec-2] Member localhost:27022 is now in state STARTUP2
2023-03-13T14:04:45.784+0000 I  STORAGE  [replexec-1] createCollection: local.temp_oplog_buffer with generated UUID: 384de2f7-3d8a-40b6-9890-815cc66385e0 and options: { temp: true }
2023-03-13T14:04:45.803+0000 I  INDEX    [replexec-1] index build: done building index _id_ on ns local.temp_oplog_buffer
2023-03-13T14:04:45.803+0000 I  INITSYNC [replication-0] Starting initial sync (attempt 1 of 10)
2023-03-13T14:04:45.803+0000 I  STORAGE  [replication-0] Finishing collection drop for local.temp_oplog_buffer (384de2f7-3d8a-40b6-9890-815cc66385e0).
2023-03-13T14:04:45.809+0000 I  STORAGE  [replication-0] createCollection: local.temp_oplog_buffer with generated UUID: d69dc494-f7e0-4631-abb9-2261f2b80b71 and options: { temp: true }
2023-03-13T14:04:45.823+0000 I  INDEX    [replication-0] index build: done building index _id_ on ns local.temp_oplog_buffer
2023-03-13T14:04:45.823+0000 I  REPL     [replication-0] waiting for 2 pings from other members before syncing
2023-03-13T14:04:46.824+0000 I  REPL     [replication-1] sync source candidate: localhost:27021
2023-03-13T14:04:46.825+0000 I  INITSYNC [replication-1] Initial syncer oplog truncation finished in: 0ms
2023-03-13T14:04:46.825+0000 I  REPL     [replication-1] ******
2023-03-13T14:04:46.825+0000 I  REPL     [replication-1] creating replication oplog of size: 6612MB...
2023-03-13T14:04:46.826+0000 I  STORAGE  [replication-1] createCollection: local.oplog.rs with generated UUID: 6f90533f-f789-4b0e-a1ca-847232cd8205 and options: { capped: true, size: 6933844377.0, autoIndexId: false }
2023-03-13T14:04:46.847+0000 I  STORAGE  [replication-1] Starting OplogTruncaterThread local.oplog.rs
2023-03-13T14:04:46.848+0000 I  STORAGE  [replication-1] The size storer reports that the oplog contains 0 records totaling to 0 bytes
2023-03-13T14:04:46.848+0000 I  STORAGE  [replication-1] Scanning the oplog to determine where to place markers for truncation
2023-03-13T14:04:46.848+0000 I  STORAGE  [replication-1] WiredTiger record store oplog processing took 0ms
2023-03-13T14:04:46.886+0000 I  REPL     [replication-1] ******
2023-03-13T14:04:46.886+0000 I  REPL     [replication-1] dropReplicatedDatabases - dropping 1 databases
2023-03-13T14:04:46.887+0000 I  REPL     [replication-1] dropReplicatedDatabases - dropped 1 databases
2023-03-13T14:04:46.887+0000 I  CONNPOOL [RS] Connecting to localhost:27021
2023-03-13T14:04:46.906+0000 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:admin.system.version
2023-03-13T14:04:46.908+0000 I  STORAGE  [repl-writer-worker-0] createCollection: admin.system.version with provided UUID: 3c8b97b2-8533-4bd3-a6e1-e52427eba037 and options: { uuid: UUID("3c8b97b2-8533-4bd3-a6e1-e52427eba037") }
2023-03-13T14:04:46.924+0000 I  INDEX    [repl-writer-worker-0] index build: starting on admin.system.version properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.version" } using method: Foreground
2023-03-13T14:04:46.924+0000 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
2023-03-13T14:04:46.941+0000 I  COMMAND  [repl-writer-worker-0] setting featureCompatibilityVersion to 4.2
2023-03-13T14:04:46.942+0000 I  NETWORK  [repl-writer-worker-0] Skip closing connection for connection # 8
2023-03-13T14:04:46.942+0000 I  NETWORK  [repl-writer-worker-0] Skip closing connection for connection # 2
2023-03-13T14:04:46.942+0000 I  INITSYNC [replication-1] CollectionCloner ns:admin.system.version finished cloning with status: OK
2023-03-13T14:04:46.945+0000 I  INDEX    [replication-1] index build: inserted 1 keys from external sorter into index in 0 seconds
2023-03-13T14:04:46.948+0000 I  INDEX    [replication-1] index build: done building index _id_ on ns admin.system.version
2023-03-13T14:04:46.985+0000 I  INITSYNC [replication-1] Finished cloning data: OK. Beginning oplog replay.
2023-03-13T14:04:46.986+0000 I  INITSYNC [replication-0] No need to apply operations. (currently at { : Timestamp(1678716285, 1) })
2023-03-13T14:04:46.987+0000 I  COMMAND  [replication-1] CMD: collMod: { collMod: "replset.oplogTruncateAfterPoint" }
2023-03-13T14:04:46.988+0000 I  COMMAND  [replication-1] CMD: collMod: { collMod: "replset.minvalid" }
2023-03-13T14:04:46.988+0000 I  COMMAND  [replication-1] CMD: collMod: { collMod: "system.replset" }
2023-03-13T14:04:46.988+0000 I  COMMAND  [replication-1] CMD: collMod: { collMod: "oplog.rs" }
2023-03-13T14:04:46.988+0000 I  COMMAND  [replication-1] CMD: collMod: { collMod: "system.rollback.id" }
2023-03-13T14:04:46.989+0000 I  COMMAND  [replication-1] CMD: collMod: { collMod: "temp_oplog_buffer" }
2023-03-13T14:04:46.989+0000 I  COMMAND  [replication-1] CMD: collMod: { collMod: "startup_log" }
2023-03-13T14:04:46.989+0000 I  COMMAND  [replication-1] CMD: collMod: { collMod: "replset.election" }
2023-03-13T14:04:46.990+0000 I  INITSYNC [replication-0] Finished fetching oplog during initial sync: CallbackCanceled: error in fetcher batch callback: oplog fetcher is shutting down. Last fetched optime: { ts: Timestamp(0, 0), t: -1 }
2023-03-13T14:04:46.990+0000 I  INITSYNC [replication-0] Initial sync attempt finishing up.
2023-03-13T14:04:46.990+0000 I  INITSYNC [replication-0] Initial Sync Attempt Statistics: { failedInitialSyncAttempts: 0, maxFailedInitialSyncAttempts: 10, initialSyncStart: new Date(1678716285803), totalInitialSyncElapsedMillis: 1187, initialSyncAttempts: [], approxTotalDataSize: 59, approxTotalBytesCopied: 59, remainingInitialSyncEstimatedMillis: 0, fetchedMissingDocs: 0, appliedOps: 0, initialSyncOplogStart: Timestamp(1678716285, 1), initialSyncOplogEnd: Timestamp(1678716285, 1), databases: { databasesToClone: 0, databasesCloned: 1, admin: { collections: 1, clonedCollections: 1, start: new Date(1678716286903), end: new Date(1678716286986), elapsedMillis: 83, admin.system.version: { documentsToCopy: 1, documentsCopied: 1, indexes: 1, fetchedBatches: 1, bytesToCopy: 59, approxBytesCopied: 59, start: new Date(1678716286906), end: new Date(1678716286986), elapsedMillis: 80, receivedBatches: 1 } } } }
2023-03-13T14:04:46.990+0000 I  STORAGE  [replication-1] Finishing collection drop for local.temp_oplog_buffer (d69dc494-f7e0-4631-abb9-2261f2b80b71).
2023-03-13T14:04:46.991+0000 I  CONNPOOL [RS] Ending connection to host localhost:27021 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
2023-03-13T14:04:46.998+0000 I  INITSYNC [replication-1] initial sync done; took 1s.
2023-03-13T14:04:46.998+0000 I  REPL     [replication-1] transition to RECOVERING from STARTUP2
2023-03-13T14:04:46.998+0000 I  REPL     [replication-1] Starting replication fetcher thread
2023-03-13T14:04:46.998+0000 I  REPL     [replication-1] Starting replication applier thread
2023-03-13T14:04:46.999+0000 I  REPL     [replication-1] Starting replication reporter thread
2023-03-13T14:04:46.999+0000 I  REPL     [rsSync-0] Starting oplog application
2023-03-13T14:04:46.999+0000 I  REPL     [rsBackgroundSync] could not find member to sync from
2023-03-13T14:04:47.001+0000 I  REPL     [replexec-4] Member localhost:27022 is now in state RECOVERING
2023-03-13T14:04:47.005+0000 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
2023-03-13T14:04:47.005+0000 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
2023-03-13T14:04:47.502+0000 I  REPL     [replexec-2] Member localhost:27022 is now in state SECONDARY
2023-03-13T14:04:55.665+0000 I  ELECTION [conn2] Received vote request: { replSetRequestVotes: 1, setName: "myRepl", dryRun: true, term: 0, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1678716285, 1), t: -1 } }
2023-03-13T14:04:55.665+0000 I  ELECTION [conn2] Sending vote response: { term: 0, voteGranted: true, reason: "" }
2023-03-13T14:04:55.667+0000 I  NETWORK  [conn2] end connection 127.0.0.1:51915 (1 connection now open)
2023-03-13T14:04:55.670+0000 I  NETWORK  [listener] connection accepted from 127.0.0.1:51938 #13 (2 connections now open)
2023-03-13T14:04:55.671+0000 I  NETWORK  [conn13] received client metadata from 127.0.0.1:51938 conn13: { driver: { name: "NetworkInterfaceTL", version: "4.2.24" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
2023-03-13T14:04:55.679+0000 I  ELECTION [conn13] Received vote request: { replSetRequestVotes: 1, setName: "myRepl", dryRun: false, term: 1, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp(1678716285, 1), t: -1 } }
2023-03-13T14:04:55.679+0000 I  ELECTION [conn13] Sending vote response: { term: 1, voteGranted: true, reason: "" }
2023-03-13T14:04:55.684+0000 I  NETWORK  [conn13] end connection 127.0.0.1:51938 (1 connection now open)
2023-03-13T14:04:55.686+0000 I  NETWORK  [listener] connection accepted from 127.0.0.1:51939 #14 (2 connections now open)
2023-03-13T14:04:55.687+0000 I  NETWORK  [conn14] received client metadata from 127.0.0.1:51939 conn14: { driver: { name: "NetworkInterfaceTL", version: "4.2.24" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
2023-03-13T14:04:56.028+0000 I  REPL     [replexec-4] Member localhost:27021 is now in state PRIMARY
2023-03-13T14:04:57.018+0000 I  REPL     [rsBackgroundSync] sync source candidate: localhost:27021
2023-03-13T14:04:57.030+0000 I  REPL     [rsBackgroundSync] Changed sync source from empty to localhost:27021
2023-03-13T14:04:57.032+0000 I  CONNPOOL [RS] Connecting to localhost:27021
2023-03-13T14:04:57.038+0000 I  STORAGE  [repl-writer-worker-0] createCollection: config.transactions with provided UUID: 4bc16f6a-b378-43ad-9963-f32215cf50b9 and options: { uuid: UUID("4bc16f6a-b378-43ad-9963-f32215cf50b9") }
2023-03-13T14:04:57.063+0000 I  INDEX    [repl-writer-worker-0] index build: done building index _id_ on ns config.transactions
2023-03-13T14:04:57.066+0000 I  STORAGE  [repl-writer-worker-0] createCollection: config.image_collection with provided UUID: 3bbbdab8-7692-4202-8099-c5d79cd0ae94 and options: { uuid: UUID("3bbbdab8-7692-4202-8099-c5d79cd0ae94") }
2023-03-13T14:04:57.082+0000 I  INDEX    [repl-writer-worker-0] index build: done building index _id_ on ns config.image_collection
2023-03-13T14:04:57.087+0000 I  STORAGE  [repl-writer-worker-0] createCollection: config.chunks with provided UUID: 4b9dce1e-4745-4687-b714-450de200d7c5 and options: { uuid: UUID("4b9dce1e-4745-4687-b714-450de200d7c5") }
2023-03-13T14:04:57.106+0000 I  INDEX    [repl-writer-worker-0] index build: done building index _id_ on ns config.chunks
2023-03-13T14:04:57.141+0000 I  INDEX    [repl-writer-worker-0] index build: starting on config.chunks properties: { v: 2, unique: true, key: { ns: 1, min: 1 }, name: "ns_1_min_1", ns: "config.chunks" } using method: Hybrid
2023-03-13T14:04:57.141+0000 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
2023-03-13T14:04:57.142+0000 I  STORAGE  [repl-writer-worker-0] Index build initialized: 12407d8e-dc36-4142-bb00-0c5c94fc3e6b: config.chunks (4b9dce1e-4745-4687-b714-450de200d7c5 ): indexes: 1
2023-03-13T14:04:57.144+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2023-03-13T14:04:57.149+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2023-03-13T14:04:57.152+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index ns_1_min_1 on ns config.chunks
2023-03-13T14:04:57.158+0000 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: 12407d8e-dc36-4142-bb00-0c5c94fc3e6b: config.chunks ( 4b9dce1e-4745-4687-b714-450de200d7c5 ). Index specs built: 1. Indexes in catalog before build: 1. Indexes in catalog after build: 2
2023-03-13T14:04:57.194+0000 I  INDEX    [repl-writer-worker-0] index build: starting on config.chunks properties: { v: 2, unique: true, key: { ns: 1, shard: 1, min: 1 }, name: "ns_1_shard_1_min_1", ns: "config.chunks" } using method: Hybrid
2023-03-13T14:04:57.194+0000 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
2023-03-13T14:04:57.195+0000 I  STORAGE  [repl-writer-worker-0] Index build initialized: 6a17a333-e68d-424c-8ddb-4c49522bd759: config.chunks (4b9dce1e-4745-4687-b714-450de200d7c5 ): indexes: 1
2023-03-13T14:04:57.195+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2023-03-13T14:04:57.199+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2023-03-13T14:04:57.203+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index ns_1_shard_1_min_1 on ns config.chunks
2023-03-13T14:04:57.209+0000 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: 6a17a333-e68d-424c-8ddb-4c49522bd759: config.chunks ( 4b9dce1e-4745-4687-b714-450de200d7c5 ). Index specs built: 1. Indexes in catalog before build: 2. Indexes in catalog after build: 3
2023-03-13T14:04:57.232+0000 I  INDEX    [repl-writer-worker-0] index build: starting on config.chunks properties: { v: 2, unique: true, key: { ns: 1, lastmod: 1 }, name: "ns_1_lastmod_1", ns: "config.chunks" } using method: Hybrid
2023-03-13T14:04:57.232+0000 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
2023-03-13T14:04:57.232+0000 I  STORAGE  [repl-writer-worker-0] Index build initialized: 5fc49362-21d9-4019-b285-d791587b3f65: config.chunks (4b9dce1e-4745-4687-b714-450de200d7c5 ): indexes: 1
2023-03-13T14:04:57.233+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2023-03-13T14:04:57.235+0000 I  STORAGE  [repl-writer-worker-0] createCollection: config.migrations with provided UUID: 49bda865-42e1-486b-b6eb-abe1f7b337c8 and options: { uuid: UUID("49bda865-42e1-486b-b6eb-abe1f7b337c8") }
2023-03-13T14:04:57.236+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2023-03-13T14:04:57.244+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index ns_1_lastmod_1 on ns config.chunks
2023-03-13T14:04:57.254+0000 I  INDEX    [repl-writer-worker-0] index build: done building index _id_ on ns config.migrations
2023-03-13T14:04:57.264+0000 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: 5fc49362-21d9-4019-b285-d791587b3f65: config.chunks ( 4b9dce1e-4745-4687-b714-450de200d7c5 ). Index specs built: 1. Indexes in catalog before build: 3. Indexes in catalog after build: 4
2023-03-13T14:04:57.295+0000 I  INDEX    [repl-writer-worker-0] index build: starting on config.migrations properties: { v: 2, unique: true, key: { ns: 1, min: 1 }, name: "ns_1_min_1", ns: "config.migrations" } using method: Hybrid
2023-03-13T14:04:57.295+0000 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
2023-03-13T14:04:57.295+0000 I  STORAGE  [repl-writer-worker-0] Index build initialized: 1dd2d40a-1623-41e7-a76c-115b461b4f0c: config.migrations (49bda865-42e1-486b-b6eb-abe1f7b337c8 ): indexes: 1
2023-03-13T14:04:57.295+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2023-03-13T14:04:57.298+0000 I  STORAGE  [repl-writer-worker-0] createCollection: config.shards with provided UUID: 574a323a-ec5e-4441-82b6-a39100488862 and options: { uuid: UUID("574a323a-ec5e-4441-82b6-a39100488862") }
2023-03-13T14:04:57.299+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2023-03-13T14:04:57.309+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index ns_1_min_1 on ns config.migrations
2023-03-13T14:04:57.321+0000 I  INDEX    [repl-writer-worker-0] index build: done building index _id_ on ns config.shards
2023-03-13T14:04:57.326+0000 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: 1dd2d40a-1623-41e7-a76c-115b461b4f0c: config.migrations ( 49bda865-42e1-486b-b6eb-abe1f7b337c8 ). Index specs built: 1. Indexes in catalog before build: 1. Indexes in catalog after build: 2
2023-03-13T14:04:57.358+0000 I  INDEX    [repl-writer-worker-0] index build: starting on config.shards properties: { v: 2, unique: true, key: { host: 1 }, name: "host_1", ns: "config.shards" } using method: Hybrid
2023-03-13T14:04:57.358+0000 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
2023-03-13T14:04:57.358+0000 I  STORAGE  [repl-writer-worker-0] Index build initialized: 1884b545-dc44-41f6-9432-863fb94b1c81: config.shards (574a323a-ec5e-4441-82b6-a39100488862 ): indexes: 1
2023-03-13T14:04:57.359+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2023-03-13T14:04:57.361+0000 I  STORAGE  [repl-writer-worker-0] createCollection: config.locks with provided UUID: 03465cc4-68ee-4658-8da3-8fd152bcbb9b and options: { uuid: UUID("03465cc4-68ee-4658-8da3-8fd152bcbb9b") }
2023-03-13T14:04:57.363+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2023-03-13T14:04:57.374+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index host_1 on ns config.shards
2023-03-13T14:04:57.384+0000 I  INDEX    [repl-writer-worker-0] index build: done building index _id_ on ns config.locks
2023-03-13T14:04:57.396+0000 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: 1884b545-dc44-41f6-9432-863fb94b1c81: config.shards ( 574a323a-ec5e-4441-82b6-a39100488862 ). Index specs built: 1. Indexes in catalog before build: 1. Indexes in catalog after build: 2
2023-03-13T14:04:57.419+0000 I  INDEX    [repl-writer-worker-0] index build: starting on config.locks properties: { v: 2, key: { ts: 1 }, name: "ts_1", ns: "config.locks" } using method: Hybrid
2023-03-13T14:04:57.419+0000 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
2023-03-13T14:04:57.420+0000 I  STORAGE  [repl-writer-worker-0] Index build initialized: f8b993a7-af46-42e2-980c-458d0dc31f26: config.locks (03465cc4-68ee-4658-8da3-8fd152bcbb9b ): indexes: 1
2023-03-13T14:04:57.420+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2023-03-13T14:04:57.424+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2023-03-13T14:04:57.428+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index ts_1 on ns config.locks
2023-03-13T14:04:57.430+0000 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: f8b993a7-af46-42e2-980c-458d0dc31f26: config.locks ( 03465cc4-68ee-4658-8da3-8fd152bcbb9b ). Index specs built: 1. Indexes in catalog before build: 1. Indexes in catalog after build: 2
2023-03-13T14:04:57.447+0000 I  INDEX    [repl-writer-worker-0] index build: starting on config.locks properties: { v: 2, key: { state: 1, process: 1 }, name: "state_1_process_1", ns: "config.locks" } using method: Hybrid
2023-03-13T14:04:57.447+0000 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
2023-03-13T14:04:57.448+0000 I  STORAGE  [repl-writer-worker-0] Index build initialized: 9568ff53-bf07-499a-937d-3357b5395e84: config.locks (03465cc4-68ee-4658-8da3-8fd152bcbb9b ): indexes: 1
2023-03-13T14:04:57.448+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2023-03-13T14:04:57.450+0000 I  STORAGE  [repl-writer-worker-0] createCollection: config.lockpings with provided UUID: bc6b9c2f-e4ee-4c3d-b3c6-a07cd0dea5bb and options: { uuid: UUID("bc6b9c2f-e4ee-4c3d-b3c6-a07cd0dea5bb") }
2023-03-13T14:04:57.451+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2023-03-13T14:04:57.461+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index state_1_process_1 on ns config.locks
2023-03-13T14:04:57.464+0000 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: 9568ff53-bf07-499a-937d-3357b5395e84: config.locks ( 03465cc4-68ee-4658-8da3-8fd152bcbb9b ). Index specs built: 1. Indexes in catalog before build: 2. Indexes in catalog after build: 3
2023-03-13T14:04:57.471+0000 I  INDEX    [repl-writer-worker-0] index build: done building index _id_ on ns config.lockpings
2023-03-13T14:04:57.492+0000 I  INDEX    [repl-writer-worker-0] index build: starting on config.lockpings properties: { v: 2, key: { ping: 1 }, name: "ping_1", ns: "config.lockpings" } using method: Hybrid
2023-03-13T14:04:57.492+0000 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
2023-03-13T14:04:57.493+0000 I  STORAGE  [repl-writer-worker-0] Index build initialized: 15fc985d-bf8f-48b7-9337-e8b3a5c8a671: config.lockpings (bc6b9c2f-e4ee-4c3d-b3c6-a07cd0dea5bb ): indexes: 1
2023-03-13T14:04:57.493+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2023-03-13T14:04:57.496+0000 I  STORAGE  [repl-writer-worker-0] createCollection: config.tags with provided UUID: 88d57bbe-e3d1-436e-9581-5d7be154e21b and options: { uuid: UUID("88d57bbe-e3d1-436e-9581-5d7be154e21b") }
2023-03-13T14:04:57.497+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2023-03-13T14:04:57.502+0000 I  STORAGE  [replication-1] Triggering the first stable checkpoint. Initial Data: Timestamp(1678716285, 1) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1678716296, 6)
2023-03-13T14:04:57.509+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index ping_1 on ns config.lockpings
2023-03-13T14:04:57.519+0000 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: 15fc985d-bf8f-48b7-9337-e8b3a5c8a671: config.lockpings ( bc6b9c2f-e4ee-4c3d-b3c6-a07cd0dea5bb ). Index specs built: 1. Indexes in catalog before build: 1. Indexes in catalog after build: 2
2023-03-13T14:04:57.520+0000 I  INDEX    [repl-writer-worker-0] index build: done building index _id_ on ns config.tags
2023-03-13T14:04:57.560+0000 I  INDEX    [repl-writer-worker-0] index build: starting on config.tags properties: { v: 2, unique: true, key: { ns: 1, min: 1 }, name: "ns_1_min_1", ns: "config.tags" } using method: Hybrid
2023-03-13T14:04:57.560+0000 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
2023-03-13T14:04:57.560+0000 I  STORAGE  [repl-writer-worker-0] Index build initialized: 02ee37af-40b2-4ec5-9b24-586f3d0eb625: config.tags (88d57bbe-e3d1-436e-9581-5d7be154e21b ): indexes: 1
2023-03-13T14:04:57.560+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2023-03-13T14:04:57.567+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2023-03-13T14:04:57.571+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index ns_1_min_1 on ns config.tags
2023-03-13T14:04:57.585+0000 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: 02ee37af-40b2-4ec5-9b24-586f3d0eb625: config.tags ( 88d57bbe-e3d1-436e-9581-5d7be154e21b ). Index specs built: 1. Indexes in catalog before build: 1. Indexes in catalog after build: 2
2023-03-13T14:04:57.610+0000 I  INDEX    [repl-writer-worker-0] index build: starting on config.tags properties: { v: 2, key: { ns: 1, tag: 1 }, name: "ns_1_tag_1", ns: "config.tags" } using method: Hybrid
2023-03-13T14:04:57.610+0000 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
2023-03-13T14:04:57.610+0000 I  STORAGE  [repl-writer-worker-0] Index build initialized: a3a87a62-4d95-48cc-93d9-c3fb57ed38d9: config.tags (88d57bbe-e3d1-436e-9581-5d7be154e21b ): indexes: 1
2023-03-13T14:04:57.610+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: collection scan done. scanned 0 total records in 0 seconds
2023-03-13T14:04:57.613+0000 I  STORAGE  [repl-writer-worker-0] createCollection: config.version with provided UUID: d81b8cf0-bfde-4205-8a3a-b26ab8ee7f11 and options: { uuid: UUID("d81b8cf0-bfde-4205-8a3a-b26ab8ee7f11") }
2023-03-13T14:04:57.614+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: inserted 0 keys from external sorter into index in 0 seconds
2023-03-13T14:04:57.623+0000 I  INDEX    [IndexBuildsCoordinatorMongod-0] index build: done building index ns_1_tag_1 on ns config.tags
2023-03-13T14:04:57.631+0000 I  INDEX    [repl-writer-worker-0] index build: done building index _id_ on ns config.version
2023-03-13T14:04:57.634+0000 I  STORAGE  [IndexBuildsCoordinatorMongod-0] Index build completed successfully: a3a87a62-4d95-48cc-93d9-c3fb57ed38d9: config.tags ( 88d57bbe-e3d1-436e-9581-5d7be154e21b ). Index specs built: 1. Indexes in catalog before build: 2. Indexes in catalog after build: 3
2023-03-13T14:04:57.663+0000 I  STORAGE  [repl-writer-worker-0] createCollection: admin.system.keys with provided UUID: 8a3fa9ad-5efc-4207-83e6-53d314d03c88 and options: { uuid: UUID("8a3fa9ad-5efc-4207-83e6-53d314d03c88") }
2023-03-13T14:04:57.685+0000 I  INDEX    [repl-writer-worker-0] index build: done building index _id_ on ns admin.system.keys
2023-03-13T14:07:11.658+0000 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: Collection config.system.sessions is not sharded.
2023-03-13T14:07:11.658+0000 I  CONTROL  [LogicalSessionCacheRefresh] Failed to create config.system.sessions: Cannot create config.system.sessions until there are shards, will try again at the next refresh interval
2023-03-13T14:07:11.659+0000 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Cannot create config.system.sessions until there are shards
2023-03-13T14:09:29.456+0000 I  NETWORK  [listener] connection accepted from 127.0.0.1:57914 #16 (3 connections now open)
2023-03-13T14:09:29.457+0000 I  NETWORK  [conn16] end connection 127.0.0.1:57914 (2 connections now open)
2023-03-13T14:09:29.458+0000 I  NETWORK  [conn14] end connection 127.0.0.1:51939 (1 connection now open)
2023-03-13T14:09:29.459+0000 I  NETWORK  [listener] connection accepted from 127.0.0.1:57915 #17 (2 connections now open)
2023-03-13T14:09:29.461+0000 I  NETWORK  [conn17] received client metadata from 127.0.0.1:57915 conn17: { driver: { name: "NetworkInterfaceTL", version: "4.2.24" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
2023-03-13T14:09:29.465+0000 I  REPL     [replication-1] Choosing new sync source because the config version supplied by localhost:27021, 2, does not match ours, 1
2023-03-13T14:09:29.465+0000 I  REPL     [replication-1] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: localhost:27021, OpTime { ts: Timestamp(1678716569, 1), t: 1 }, its sync source index:-1
2023-03-13T14:09:29.465+0000 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source localhost:27021 (config version: 2; last applied optime: { ts: Timestamp(1678716569, 1), t: 1 }; sync source index: -1; primary index: 0) is no longer valid
2023-03-13T14:09:29.465+0000 I  REPL     [rsBackgroundSync] Clearing sync source localhost:27021 to choose a new one.
2023-03-13T14:09:29.466+0000 I  REPL     [rsBackgroundSync] could not find member to sync from
2023-03-13T14:09:29.466+0000 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to localhost:27021: InvalidSyncSource: Sync source was cleared. Was localhost:27021
2023-03-13T14:09:29.472+0000 I  NETWORK  [listener] connection accepted from 127.0.0.1:57919 #20 (3 connections now open)
2023-03-13T14:09:29.473+0000 I  NETWORK  [conn20] end connection 127.0.0.1:57919 (2 connections now open)
2023-03-13T14:09:29.474+0000 I  REPL     [replexec-1] New replica set config in use: { _id: "myRepl", version: 2, configsvr: true, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "localhost:27021", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 2.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "localhost:27022", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: "localhost:27023", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('640f2d7dd54f47b778949376') } }
2023-03-13T14:09:29.474+0000 I  CONNPOOL [Replication] Ending connection to host localhost:27022 due to bad connection status: CallbackCanceled: Callback was canceled; 0 connections to that host remain open
2023-03-13T14:09:29.474+0000 I  REPL     [replexec-1] This node is localhost:27023 in the config
2023-03-13T14:09:29.474+0000 I  CONNPOOL [Replication] Connecting to localhost:27022
2023-03-13T14:09:47.484+0000 I  REPL     [rsBackgroundSync] sync source candidate: localhost:27021
2023-03-13T14:09:47.487+0000 I  REPL     [rsBackgroundSync] Changed sync source from empty to localhost:27021
2023-03-13T14:12:11.658+0000 I  SH_REFR  [ConfigServerCatalogCacheLoader-1] Refresh for collection config.system.sessions took 0 ms and found the collection is not sharded
2023-03-13T14:12:11.658+0000 I  CONTROL  [LogicalSessionCacheRefresh] Failed to create config.system.sessions: Cannot create config.system.sessions until there are shards, will try again at the next refresh interval
2023-03-13T14:12:11.658+0000 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: Collection config.system.sessions is not sharded.
2023-03-13T14:12:11.658+0000 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Cannot create config.system.sessions until there are shards
2023-03-13T14:17:11.659+0000 I  SH_REFR  [ConfigServerCatalogCacheLoader-2] Refresh for collection config.system.sessions took 1 ms and found the collection is not sharded
2023-03-13T14:17:11.660+0000 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: Collection config.system.sessions is not sharded.
2023-03-13T14:17:11.660+0000 I  CONTROL  [LogicalSessionCacheRefresh] Failed to create config.system.sessions: Cannot create config.system.sessions until there are shards, will try again at the next refresh interval
2023-03-13T14:17:11.660+0000 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Cannot create config.system.sessions until there are shards
2023-03-13T14:19:07.479+0000 I  NETWORK  [listener] connection accepted from 127.0.0.1:60593 #22 (3 connections now open)
2023-03-13T14:19:07.482+0000 I  NETWORK  [conn22] received client metadata from 127.0.0.1:60593 conn22: { driver: { name: "NetworkInterfaceTL", version: "4.2.24" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
2023-03-13T14:19:07.489+0000 I  NETWORK  [listener] connection accepted from 127.0.0.1:60596 #23 (4 connections now open)
2023-03-13T14:19:07.490+0000 I  NETWORK  [conn23] received client metadata from 127.0.0.1:60596 conn23: { driver: { name: "NetworkInterfaceTL", version: "4.2.24" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
2023-03-13T14:19:08.073+0000 I  STORAGE  [repl-writer-worker-1] createCollection: config.mongos with provided UUID: c6f32de2-f98e-4177-95de-64009efc95d1 and options: { uuid: UUID("c6f32de2-f98e-4177-95de-64009efc95d1") }
2023-03-13T14:19:08.094+0000 I  INDEX    [repl-writer-worker-1] index build: done building index _id_ on ns config.mongos
2023-03-13T14:20:39.719+0000 I  NETWORK  [conn22] end connection 127.0.0.1:60593 (3 connections now open)
2023-03-13T14:20:39.728+0000 I  NETWORK  [conn23] end connection 127.0.0.1:60596 (2 connections now open)
2023-03-13T14:20:58.958+0000 I  NETWORK  [listener] connection accepted from 127.0.0.1:60604 #24 (3 connections now open)
2023-03-13T14:20:58.960+0000 I  NETWORK  [conn24] received client metadata from 127.0.0.1:60604 conn24: { driver: { name: "NetworkInterfaceTL", version: "4.2.24" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
2023-03-13T14:20:59.974+0000 I  NETWORK  [listener] connection accepted from 127.0.0.1:60608 #25 (4 connections now open)
2023-03-13T14:20:59.975+0000 I  NETWORK  [conn25] received client metadata from 127.0.0.1:60608 conn25: { driver: { name: "NetworkInterfaceTL", version: "4.2.24" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
2023-03-13T14:21:53.409+0000 I  NETWORK  [listener] connection accepted from 127.0.0.1:60612 #26 (5 connections now open)
2023-03-13T14:21:53.410+0000 I  NETWORK  [conn26] received client metadata from 127.0.0.1:60612 conn26: { driver: { name: "NetworkInterfaceTL", version: "4.2.24" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
2023-03-13T14:21:53.419+0000 I  NETWORK  [listener] connection accepted from 127.0.0.1:60617 #27 (6 connections now open)
2023-03-13T14:21:53.420+0000 I  NETWORK  [conn27] received client metadata from 127.0.0.1:60617 conn27: { driver: { name: "NetworkInterfaceTL", version: "4.2.24" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
2023-03-13T14:22:00.251+0000 I  NETWORK  [listener] connection accepted from 127.0.0.1:60619 #28 (7 connections now open)
2023-03-13T14:22:00.251+0000 I  NETWORK  [conn28] received client metadata from 127.0.0.1:60619 conn28: { driver: { name: "NetworkInterfaceTL", version: "4.2.24" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
2023-03-13T14:22:02.281+0000 I  NETWORK  [listener] connection accepted from 127.0.0.1:60626 #29 (8 connections now open)
2023-03-13T14:22:02.282+0000 I  NETWORK  [conn29] received client metadata from 127.0.0.1:60626 conn29: { driver: { name: "NetworkInterfaceTL", version: "4.2.24" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
2023-03-13T14:22:11.658+0000 I  SH_REFR  [ConfigServerCatalogCacheLoader-3] Refresh for collection config.system.sessions took 1 ms and found the collection is not sharded
2023-03-13T14:22:11.659+0000 I  CONTROL  [LogicalSessionCacheRefresh] Failed to create config.system.sessions: Cannot create config.system.sessions until there are shards, will try again at the next refresh interval
2023-03-13T14:22:11.659+0000 I  CONTROL  [LogicalSessionCacheRefresh] Sessions collection is not set up; waiting until next sessions refresh interval: Cannot create config.system.sessions until there are shards
2023-03-13T14:22:11.660+0000 I  SH_REFR  [ConfigServerCatalogCacheLoader-3] Refresh for collection config.system.sessions took 0 ms and found the collection is not sharded
2023-03-13T14:22:11.660+0000 I  CONTROL  [LogicalSessionCacheReap] Sessions collection is not set up; waiting until next sessions reap interval: Collection config.system.sessions is not sharded.
2023-03-13T14:24:06.567+0000 I  NETWORK  [conn24] end connection 127.0.0.1:60604 (7 connections now open)
2023-03-13T14:24:06.569+0000 I  NETWORK  [conn25] end connection 127.0.0.1:60608 (6 connections now open)
2023-03-13T14:24:08.486+0000 I  NETWORK  [conn26] end connection 127.0.0.1:60612 (5 connections now open)
2023-03-13T14:24:08.489+0000 I  NETWORK  [conn27] end connection 127.0.0.1:60617 (4 connections now open)
2023-03-13T14:24:10.299+0000 I  NETWORK  [conn28] end connection 127.0.0.1:60619 (3 connections now open)
2023-03-13T14:24:10.303+0000 I  NETWORK  [conn29] end connection 127.0.0.1:60626 (2 connections now open)
2023-03-13T14:24:20.537+0000 I  ELECTION [conn8] Received vote request: { replSetRequestVotes: 1, setName: "myRepl", dryRun: false, term: 2, candidateIndex: 1, configVersion: 2, lastCommittedOp: { ts: Timestamp(1678717456, 1), t: 1 } }
2023-03-13T14:24:20.537+0000 I  ELECTION [conn8] Sending vote response: { term: 2, voteGranted: true, reason: "" }
2023-03-13T14:24:21.297+0000 I  REPL     [replication-2] Choosing new sync source. Our current sync source is not primary and does not have a sync source, so we require that it is ahead of us. Current sync source: localhost:27021, my last fetched oplog optime: { ts: Timestamp(1678717456, 1), t: 1 }, latest oplog optime of sync source: { ts: Timestamp(1678717456, 1), t: 1 } (sync source does not know the primary)
2023-03-13T14:24:21.298+0000 I  REPL     [replication-2] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: localhost:27021, OpTime { ts: Timestamp(1678717456, 1), t: 1 }, its sync source index:-1
2023-03-13T14:24:21.298+0000 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source localhost:27021 (config version: 2; last applied optime: { ts: Timestamp(1678717456, 1), t: 1 }; sync source index: -1; primary index: -1) is no longer valid
2023-03-13T14:24:21.299+0000 I  REPL     [rsBackgroundSync] Clearing sync source localhost:27021 to choose a new one.
2023-03-13T14:24:21.299+0000 I  REPL     [rsBackgroundSync] could not find member to sync from
2023-03-13T14:24:21.302+0000 I  REPL     [replexec-11] Member localhost:27021 is now in state SECONDARY
2023-03-13T14:24:21.303+0000 I  REPL     [replexec-15] Member localhost:27022 is now in state PRIMARY
2023-03-13T14:24:21.537+0000 I  NETWORK  [conn17] end connection 127.0.0.1:57915 (1 connection now open)
2023-03-13T14:24:21.810+0000 W  NETWORK  [replexec-17] Failed to check socket connectivity: L’opération a réussi.
2023-03-13T14:24:21.811+0000 I  CONNPOOL [replexec-17] dropping unhealthy pooled connection to localhost:27021
2023-03-13T14:24:21.813+0000 I  CONNPOOL [Replication] Connecting to localhost:27021
2023-03-13T14:24:22.301+0000 I  REPL     [rsBackgroundSync] sync source candidate: localhost:27022
2023-03-13T14:24:22.301+0000 I  CONNPOOL [RS] Connecting to localhost:27022
2023-03-13T14:24:22.306+0000 I  REPL     [rsBackgroundSync] Changed sync source from empty to localhost:27022
2023-03-13T14:24:22.307+0000 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to localhost:27021: InvalidSyncSource: Sync source changed from localhost:27021 to localhost:27022
2023-03-13T14:24:22.832+0000 I  COMMAND  [conn8] Received replSetStepUp request
2023-03-13T14:24:22.833+0000 I  ELECTION [conn8] Starting an election due to step up request
2023-03-13T14:24:22.833+0000 I  ELECTION [conn8] skipping dry run and running for election in term 3
2023-03-13T14:24:22.835+0000 I  REPL     [replexec-15] Scheduling remote command request for vote request: RemoteCommand 2381 -- target:localhost:27021 db:admin cmd:{ replSetRequestVotes: 1, setName: "myRepl", dryRun: false, term: 3, candidateIndex: 2, configVersion: 2, lastCommittedOp: { ts: Timestamp(1678717460, 2), t: 2 } }
2023-03-13T14:24:22.835+0000 I  REPL     [replexec-15] Scheduling remote command request for vote request: RemoteCommand 2382 -- target:localhost:27022 db:admin cmd:{ replSetRequestVotes: 1, setName: "myRepl", dryRun: false, term: 3, candidateIndex: 2, configVersion: 2, lastCommittedOp: { ts: Timestamp(1678717460, 2), t: 2 } }
2023-03-13T14:24:22.836+0000 I  CONNPOOL [Replication] Connecting to localhost:27021
2023-03-13T14:24:22.836+0000 I  ELECTION [replexec-13] VoteRequester(term 3) received an invalid response from localhost:27022: ShutdownInProgress: In the process of shutting down; response message: { operationTime: Timestamp(1678717460, 2), ok: 0.0, errmsg: "In the process of shutting down", code: 91, codeName: "ShutdownInProgress", $gleStats: { lastOpTime: Timestamp(0, 0), electionId: ObjectId('7fffffff0000000000000002') }, lastCommittedOpTime: Timestamp(1678717460, 2), $clusterTime: { clusterTime: Timestamp(1678717460, 2), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } } }
2023-03-13T14:24:23.845+0000 I  REPL     [replication-1] Restarting oplog query due to error: InterruptedAtShutdown: error in fetcher batch callback :: caused by :: interrupted at shutdown. Last fetched optime: { ts: Timestamp(1678717460, 2), t: 2 }. Restarts remaining: 1
2023-03-13T14:24:23.846+0000 I  REPL     [replication-1] Scheduled new oplog query Fetcher source: localhost:27022 database: local query: { find: "oplog.rs", filter: { ts: { $gte: Timestamp(1678717460, 2) } }, tailable: true, oplogReplay: true, awaitData: true, maxTimeMS: 2000, batchSize: 13981010, term: 3, readConcern: { afterClusterTime: Timestamp(0, 1) } } query metadata: { $replData: 1, $oplogQueryData: 1, $readPreference: { mode: "secondaryPreferred" } } active: 1 findNetworkTimeout: 7000ms getMoreNetworkTimeout: 10000ms shutting down?: 0 first: 1 firstCommandScheduler: RemoteCommandRetryScheduler request: RemoteCommand 2383 -- target:localhost:27022 db:local cmd:{ find: "oplog.rs", filter: { ts: { $gte: Timestamp(1678717460, 2) } }, tailable: true, oplogReplay: true, awaitData: true, maxTimeMS: 2000, batchSize: 13981010, term: 3, readConcern: { afterClusterTime: Timestamp(0, 1) } } active: 1 callbackHandle.valid: 1 callbackHandle.cancelled: 0 attempt: 1 retryPolicy: {type: "NoRetryPolicy"}
2023-03-13T14:24:23.846+0000 I  REPL     [replication-2] Error returned from oplog query (no more query restarts left): InterruptedAtShutdown: error in fetcher batch callback :: caused by :: interrupted at shutdown
2023-03-13T14:24:23.846+0000 W  REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InterruptedAtShutdown: error in fetcher batch callback :: caused by :: interrupted at shutdown
2023-03-13T14:24:23.846+0000 I  REPL     [rsBackgroundSync] Clearing sync source localhost:27022 to choose a new one.
2023-03-13T14:24:23.846+0000 I  REPL     [rsBackgroundSync] could not find member to sync from
2023-03-13T14:24:23.848+0000 I  REPL_HB  [replexec-15] Heartbeat to localhost:27022 failed after 2 retries, response status: InterruptedAtShutdown: interrupted at shutdown
2023-03-13T14:24:23.848+0000 I  REPL     [replexec-15] Member localhost:27022 is now in state RS_DOWN - interrupted at shutdown
2023-03-13T14:24:23.852+0000 I  CONNPOOL [Replication] Dropping all pooled connections to localhost:27021 due to HostUnreachable: Error connecting to localhost:27021 (127.0.0.1:27021) :: caused by :: Aucune connexion na pu tre tablie car lordinateur cible la expressment refuse.
2023-03-13T14:24:23.852+0000 I  ELECTION [replexec-11] VoteRequester(term 3) failed to receive response from localhost:27021: HostUnreachable: Error connecting to localhost:27021 (127.0.0.1:27021) :: caused by :: Aucune connexion na pu tre tablie car lordinateur cible la expressment refuse.
2023-03-13T14:24:23.853+0000 I  CONNPOOL [Replication] Connecting to localhost:27021
2023-03-13T14:24:23.853+0000 I  ELECTION [replexec-17] not becoming primary, we received insufficient votes
2023-03-13T14:24:23.853+0000 I  ELECTION [replexec-17] Lost election
2023-03-13T14:24:23.853+0000 I  COMMAND  [conn8] replSetStepUp request failed :: caused by :: CommandFailed: Election failed.
2023-03-13T14:24:23.853+0000 I  COMMAND  [conn8] command admin.$cmd command: replSetStepUp { replSetStepUp: 1, skipDryRun: true, $clusterTime: { clusterTime: Timestamp(1678717460, 2), signature: { hash: BinData(0, D3852D39E816AAB3D8205AF8CFA0104849E4538D), keyId: 7210031590582255629 } }, $db: "admin" } numYields:0 ok:0 errMsg:"Election failed." errName:CommandFailed errCode:125 reslen:319 locks:{} protocol:op_msg 1021ms
2023-03-13T14:24:23.854+0000 I  NETWORK  [conn8] end connection 127.0.0.1:51923 (0 connections now open)
2023-03-13T14:24:24.348+0000 W  NETWORK  [replexec-15] Failed to check socket connectivity: L’opération a réussi.
2023-03-13T14:24:24.348+0000 I  CONNPOOL [replexec-15] dropping unhealthy pooled connection to localhost:27022
2023-03-13T14:24:24.349+0000 I  CONNPOOL [Replication] Connecting to localhost:27022
2023-03-13T14:24:24.857+0000 I  CONNPOOL [Replication] Dropping all pooled connections to localhost:27021 due to HostUnreachable: Error connecting to localhost:27021 (127.0.0.1:27021) :: caused by :: Aucune connexion na pu tre tablie car lordinateur cible la expressment refuse.
2023-03-13T14:24:24.858+0000 I  CONNPOOL [Replication] Connecting to localhost:27021
2023-03-13T14:24:25.477+0000 I  CONTROL  [thread19] CTRL_CLOSE_EVENT signal
2023-03-13T14:24:25.477+0000 I  CONTROL  [consoleTerminate] got CTRL_CLOSE_EVENT, will terminate after current cmd ends
2023-03-13T14:24:25.478+0000 I  REPL     [consoleTerminate] Stepping down the ReplicationCoordinator for shutdown, waitTime: 10000ms
2023-03-13T14:24:25.478+0000 I  SHARDING [consoleTerminate] Shutting down the WaitForMajorityService
2023-03-13T14:24:25.478+0000 I  SHARDING [consoleTerminate] Shutting down the balancer
2023-03-13T14:24:25.479+0000 I  CONTROL  [consoleTerminate] Shutting down the LogicalSessionCache
2023-03-13T14:24:25.479+0000 I  NETWORK  [consoleTerminate] shutdown: going to close listening sockets...
2023-03-13T14:24:25.481+0000 I  NETWORK  [consoleTerminate] Shutting down the global connection pool
2023-03-13T14:24:25.481+0000 I  STORAGE  [consoleTerminate] Shutting down the FlowControlTicketholder
2023-03-13T14:24:25.481+0000 I  -        [consoleTerminate] Stopping further Flow Control ticket acquisitions.
2023-03-13T14:24:25.481+0000 I  STORAGE  [consoleTerminate] Shutting down the PeriodicThreadToAbortExpiredTransactions
2023-03-13T14:24:25.482+0000 I  STORAGE  [consoleTerminate] Shutting down the PeriodicThreadToDecreaseSnapshotHistoryIfNotNeeded
2023-03-13T14:24:25.482+0000 I  REPL     [consoleTerminate] Shutting down the ReplicationCoordinator
2023-03-13T14:24:25.482+0000 I  REPL     [consoleTerminate] shutting down replication subsystems
2023-03-13T14:24:25.482+0000 I  REPL     [consoleTerminate] Stopping replication reporter thread
2023-03-13T14:24:25.483+0000 I  REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to localhost:27022: CallbackCanceled: Reporter no longer valid
2023-03-13T14:24:25.483+0000 I  REPL     [consoleTerminate] Stopping replication fetcher thread
2023-03-13T14:24:25.483+0000 I  REPL     [consoleTerminate] Stopping replication applier thread
2023-03-13T14:24:25.848+0000 I  REPL     [rsBackgroundSync] Stopping replication producer
2023-03-13T14:24:25.887+0000 I  CONNPOOL [Replication] Dropping all pooled connections to localhost:27021 due to HostUnreachable: Error connecting to localhost:27021 (127.0.0.1:27021) :: caused by :: Aucune connexion na pu tre tablie car lordinateur cible la expressment refuse.
2023-03-13T14:24:25.888+0000 I  REPL_HB  [replexec-11] Heartbeat to localhost:27021 failed after 2 retries, response status: HostUnreachable: Error connecting to localhost:27021 (127.0.0.1:27021) :: caused by :: Aucune connexion na pu tre tablie car lordinateur cible la expressment refuse.
2023-03-13T14:24:25.888+0000 I  REPL     [replexec-11] Member localhost:27021 is now in state RS_DOWN - Error connecting to localhost:27021 (127.0.0.1:27021) :: caused by :: Aucune connexion na pu tre tablie car lordinateur cible la expressment refuse.
2023-03-13T14:24:26.312+0000 I  REPL     [rsSync-0] Finished oplog application
2023-03-13T14:24:26.312+0000 I  REPL     [consoleTerminate] Stopping replication storage threads
2023-03-13T14:24:26.313+0000 I  ASIO     [RS] Killing all outstanding egress activity.
2023-03-13T14:24:26.313+0000 I  ASIO     [RS] Killing all outstanding egress activity.
2023-03-13T14:24:26.313+0000 I  CONNPOOL [RS] Dropping all pooled connections to localhost:27022 due to ShutdownInProgress: Shutting down the connection pool
2023-03-13T14:24:26.315+0000 I  ASIO     [Replication] Killing all outstanding egress activity.
2023-03-13T14:24:26.315+0000 I  SHARDING [consoleTerminate] Shutting down the PeriodicShardedIndexConsistencyChecker
2023-03-13T14:24:26.315+0000 I  SHARDING [consoleTerminate] Shutting down the ShardingInitializationMongoD
2023-03-13T14:24:26.316+0000 I  REPL     [consoleTerminate] Enqueuing the ReplicationStateTransitionLock for shutdown
2023-03-13T14:24:26.316+0000 I  -        [consoleTerminate] Killing all operations for shutdown
2023-03-13T14:24:26.316+0000 I  COMMAND  [consoleTerminate] Shutting down all open transactions
2023-03-13T14:24:26.316+0000 I  REPL     [consoleTerminate] Acquiring the ReplicationStateTransitionLock for shutdown
2023-03-13T14:24:26.316+0000 I  INDEX    [consoleTerminate] Shutting down the IndexBuildsCoordinator
2023-03-13T14:24:26.316+0000 I  NETWORK  [consoleTerminate] Shutting down the ReplicaSetMonitor
2023-03-13T14:24:26.316+0000 I  SHARDING [consoleTerminate] Shutting down the shard registry
2023-03-13T14:24:26.316+0000 W  SHARDING [shard-registry-reload] cant reload ShardRegistry  :: caused by :: CallbackCanceled: Callback canceled
2023-03-13T14:24:26.316+0000 I  ASIO     [shard-registry-reload] Killing all outstanding egress activity.
2023-03-13T14:24:26.317+0000 I  REPL     [consoleTerminate] Shutting down the LogicalTimeValidator
2023-03-13T14:24:26.317+0000 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2023-03-13T14:24:26.317+0000 I  CONTROL  [consoleTerminate] Shutting down free monitoring
2023-03-13T14:24:26.317+0000 I  FTDC     [consoleTerminate] Shutting down full-time data capture
2023-03-13T14:24:26.317+0000 I  FTDC     [consoleTerminate] Shutting down full-time diagnostic data capture
2023-03-13T14:24:26.321+0000 I  STORAGE  [consoleTerminate] Shutting down the HealthLog
2023-03-13T14:24:26.321+0000 I  STORAGE  [consoleTerminate] Shutting down the storage engine
2023-03-13T14:24:26.321+0000 I  STORAGE  [consoleTerminate] Deregistering all the collections
2023-03-13T14:24:26.322+0000 I  STORAGE  [WTOplogJournalThread] Oplog journal thread loop shutting down
2023-03-13T14:24:26.322+0000 I  STORAGE  [consoleTerminate] Timestamp monitor shutting down
2023-03-13T14:24:26.322+0000 I  STORAGE  [consoleTerminate] WiredTigerKVEngine shutting down
2023-03-13T14:24:26.322+0000 I  STORAGE  [consoleTerminate] Shutting down session sweeper thread
2023-03-13T14:24:26.323+0000 I  STORAGE  [consoleTerminate] Finished shutting down session sweeper thread
2023-03-13T14:24:26.323+0000 I  STORAGE  [consoleTerminate] Shutting down journal flusher thread
2023-03-13T14:24:26.407+0000 I  STORAGE  [consoleTerminate] Finished shutting down journal flusher thread
2023-03-13T14:24:26.407+0000 I  STORAGE  [consoleTerminate] Shutting down checkpoint thread
2023-03-13T14:24:26.407+0000 I  STORAGE  [consoleTerminate] Finished shutting down checkpoint thread
2023-03-13T14:24:26.442+0000 I  STORAGE  [consoleTerminate] shutdown: removing fs lock...
2023-03-13T14:24:26.443+0000 I  -        [consoleTerminate] Dropping the scope cache for shutdown
2023-03-13T14:24:26.443+0000 I  CONTROL  [consoleTerminate] now exiting
2023-03-13T14:24:26.443+0000 I  CONTROL  [consoleTerminate] shutting down with code:12
